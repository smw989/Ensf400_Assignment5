**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student Names:  |   |
| M Munem Morshed |   |
| S M Wahid Chowdhury |   |
| Himel Paul |   |
| Maliha Chowdhury Adrita |   |
| Kosy Onyejemezi |   |

# Introduction

The main goal of this lab was to help students in comprehending the assessment of dependability and the use of tools such as C-SFRAT and the Reliability Demonstration Chart. Furthermore, the lab accesses into failure reliability growth testing, allowing for the examination of changes made to a product over a specific time frame. Finally, the group also discovered the importance of failure data analysis in determining approaches to prevent failures. These concepts reinforced the knowledge students had acquired in the lectures and provided the group with a comprehensive understanding of the critical role of reliability testing in software testing procedures.

# 

# Assessment Using Reliability Growth Testing 
For our assessment, we selected the CDS.DAT file. For compatibility with C-SFRAT, we converted it to a CSV, and included certain (inconsequential) fields. This is stored as CDS.csv at the root of this directory.

### Result of model comparison (Two models on top)

To complete this assignment, we chose the truncated logistic and IFR generalized Salvia and Bollinger models based on their log-likelihood values, which indicate how closely the model-generated curves fit the actual data. Additionally, these models achieved the highest mean and median critic scores among all those tested. We also confirmed their suitability through a qualitative review of the graphs.

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image5.jpg?raw=true)

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image6.jpg?raw=true)

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image1.jpg?raw=true)

### Range Analysis Result
After thoroughly reviewing the fault graph and evaluating the data's quality and characteristics, we chose to include the entire data range in our analysis. The dataset was deemed high-quality, with no significant signs of inaccuracies or errors. Importantly, the data reflects the system's performance during its operational phase, which is vital for assessing its real-world reliability. Since no clear outliers or anomalies were identified, incorporating the full dataset is well justified. This comprehensive approach allows us to consider all relevant information, providing a complete and accurate picture of the system’s reliability over time.

### Plots for failure intensity and reliability of the SUT

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image2.jpg?raw=true)

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image3.jpg?raw=true)

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image4.jpg?raw=true)

### Discussion on Decision-Making with a Target Failure Rate
Establishing a target failure rate helps define acceptable risk levels, guiding development efforts and reliability planning. By continuously tracking the actual failure rate against this target, teams can make timely maintenance decisions and adjustments, promoting consistent system reliability. It also serves as a useful metric for prioritizing tasks and informing leadership decisions.

Using C-SFRAT, we plotted different target failure rates. For instance, we set a baseline target based on the Mean Time To Failure (MTTF). Depending on project objectives and timelines, this target can be adjusted — a higher target rate can push for quicker, more intensive reliability improvements, while a lower target rate supports a longer-term approach, striking a balance between development speed and reliability goals.

### Advantages of Reliability Growth Testing:
- **Tracks Improvements Over Time**: Helps identify whether reliability is increasing through successive test cycles or development stages.

- **Data-Driven Decision Making**: Provides quantitative evidence for product release readiness or the need for further testing/improvements.

- **Supports Root Cause Analysis**: Failure patterns can help pinpoint design flaws or weak areas that need correction.

- **Model-Based Forecasting**: Predicts future reliability performance using statistical models (e.g., truncated logistic, IFR Salvia).

- **Improves Design Quality**: Encourages iterative design improvements by continuously evaluating test results.

- **Visual Insight**: Graphs and curves offer a clear, visual representation of how reliability evolves.

### Disadvantages of Reliability Growth Testing:
- **Requires Extensive Data Collection**:Needs failure data from multiple test phases, which may not always be available or affordable.

- **Time-Consuming and Costly**:Running repeated test cycles can increase project timelines and expenses.

- **May Not Capture All Real-World Conditions**:Testing environments might not fully replicate field usage, leading to gaps in reliability estimates.

- **Model Assumptions May Vary**:Reliability growth models rely on assumptions (e.g., failure rates, distributions) — inaccurate assumptions may lead to misleading conclusions.

- **Limited Use in Late Phases**:Less useful if applied too late in the development lifecycle when major design changes are no longer feasible.




# Assessment Using Reliability Demonstration Chart 

We used the failure data from Failure Report 3. Through trial and error we found the MTTF to be 500 (we understood this to be the minimum number of events for it to reach the acceptable region on the RDC graph).

The Reliability Demonstration Chart (MTTF = 500):

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image7.jpg?raw=true)

This graph shows that the data starts in the continued testing region and enters the acceptable region.


The Reliability Demonstration Chart with Double MTTF (MTTF = 1000):

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image8.jpg?raw=true)

This graph shows that the data stays in the yellow region, so more testing would have to be done as it is inconclusive.


The Reliability Demonstration Chart with Halved MTTF (MTTF = 250):

![Alt text](https://github.com/smw989/SENG438_Assignment5/blob/main/Images/image9.jpg?raw=true)

This graph shows that the data enters the acceptable region; however, the start of the data appears to be in the acceptable region as well, so perhaps continued testing would still need to be done.

### Advantages of Reliability Demonstration Chart (RDC):
- **Simple and Visual Evaluation**:Provides a clear graphical representation to determine if the system meets the target reliability level.

- **Useful with Limited Data**:Can be used effectively even when failure data is limited, making it practical for early-stage testing.

- **Quick Go/No-Go Decisions**:Helps teams decide whether a system is ready for release or needs further improvements.

- **Supports Confidence-Based Judgments**:Allows decisions based on statistical confidence levels, enhancing objectivity.

- **No Complex Modeling Needed**:Unlike growth models, RDCs don’t require intricate reliability modeling or historical data.

### Disadvantages of Reliability Demonstration Chart (RDC):

- **Not Ideal for Tracking Improvements**:RDCs are static; they show if a reliability target is met but don’t track changes over time like growth models do.

- **Limited Predictive Power**:RDCs assess current reliability but can’t forecast future performance or reliability trends.

- **Sensitive to Small Sample Sizes**:With very few failures or short test durations, results may be statistically weak or inconclusive.

- **Binary Outcome Focus**:Mainly supports pass/fail decisions — may not capture the nuance of partial progress toward reliability goals.

- **Assumes Consistent Testing Conditions**:Results can be misleading if the testing environment or workload isn’t realistic or consistent with operational use.


# 

# Comparison of Results

For the Reliability Growth Testing using C-SFRAT, the truncated logistic and IFR generalized Salvia and Bollinger models were selected based on their strong fit to the actual data, as determined by the log-likelihood method. This approach measured how closely the model-generated curves matched the observed data points. The selection was further supported by a qualitative review of the graphs, which confirmed the models' accuracy in representing system behavior. The entire dataset was used in the analysis, as it was of high quality and accurately reflected the system’s performance during its operational phase, ensuring a comprehensive evaluation of reliability growth.

Using the Reliability Demonstration Chart, we evaluated the system's reliability based on data from Failure Report 3. This involved testing the system and documenting any failures that occurred. From the results, we determined that the average time between failures (MTTF) is 500 hours. Additionally, we conducted further tests using a doubled MTTF of 1000 hours and a halved MTTF of 250 hours to compare system performance under different reliability assumptions.

# Discussion on Similarity and Differences of the Two Techniques

### Similarities:
- **Reliability Assessment**: Both methods are used to evaluate system reliability, offering insights that support informed decisions during product development, release planning, and ongoing improvement.

- **Data Analysis**: Each technique relies on analyzing failure data to assess system performance. Reliability Growth Analysis uses data from testing or operational phases to monitor how reliability improves over time, while RDC (Reliability Demonstration Chart) examines failure data to evaluate trends and determine if reliability targets are being achieved.

- **Visualization**: Both approaches include graphical tools that help visualize reliability data, making analysis and interpretation more intuitive.

### Differences:
- **Data Collection**: Reliability Growth Analysis gathers failure data across multiple test cycles or operational periods to observe reliability improvement trends. In contrast, RDC is typically applied when there is limited failure data, often from a small number of test events, and focuses on analyzing reliability based on these few observations.

- **Assessment Objective**: The main goal of Reliability Growth Analysis is to track and measure improvements in reliability throughout the development process. On the other hand, RDC is used to evaluate whether current reliability goals have been achieved based on observed failure patterns, providing a snapshot of the system’s current reliability status.

# How the team work/effort was divided and managed

Our team was divided into halves to handle part 1 (reliability growth testing) and part 2 (reliability demonstration chart). Afterwards, we verified the other half's work worked together to compare results from each part and evaluate the similarity and differences between the two techniques. We used Discord to maintain fast and convenient communication between all group members.

# 

# Difficulties encountered, challenges overcome, and lessons learned

Out of all of the labs in this course, our group feels that we encountered the most difficulties in this one. As stated above, the reliability growth testing software would not load any of the provided datasets. This led to us having to translate the Word data into .csv files, which took a lot of time. The other challenge we encountered were the instructions themselves being unclear. In some instances, it was not clear how to move from one step to the next, and the steps themselves didn’t go into much detail. In the end however, we overcame these challenges through teamwork and trial and error.

# Comments/feedback on the lab itself

Overall, our group found that this lab was a good introduction to Reliability Growth Testing and using Reliability Demonstration Charts. However some parts took longer due to the vagueness of the instructions, leaving students to figure out a lot of stuff on their own. For next time, we suggest providing failure data files that can actually be opened in the reliability growth testing software, and perhaps clearer instructions on how to convert them if needed. It would also be helpful for students if documentation for the required softwares were provided.
